{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Dependencies"
      ],
      "metadata": {
        "id": "fIVjoPjsMPgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import time\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from tqdm import tqdm\n",
        "from keras import regularizers"
      ],
      "metadata": {
        "id": "JR_5cjUbKrm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining necessary functions"
      ],
      "metadata": {
        "id": "q7PTxhv4MSNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Handland takes the mediapipe output landmarks, collates all the coordinates and flattens it to create an array of size 63\n",
        "\n",
        "def handland(results):\n",
        "    hl = np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.multi_hand_landmarks[0].landmark]).flatten() if results.multi_hand_landmarks else np.zeros(63)\n",
        "    return hl\n",
        "\n",
        "def handscore(results):\n",
        "    sc = results.multi_handedness[0].classification[0].score\n",
        "    return sc\n",
        "\n",
        "#Choose is basically the combined model. We have three different custom models and the combination of the three seems to be working better.\n",
        "\n",
        "def choose(a,b,c):\n",
        "    if a != '':\n",
        "        a = ord(a)\n",
        "        b = ord(b)\n",
        "        c = ord(c)\n",
        "        if (b-c) == 0:\n",
        "            return chr(b)\n",
        "        else:\n",
        "            return chr(a)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "\n",
        "#Augmentation Function\n",
        "\n",
        "from PIL import Image as PILImage\n",
        "def augment_image(image):\n",
        "    # Read the original image\n",
        "    original_image = image\n",
        "\n",
        "    # Convert image from BGR to RGB\n",
        "    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Initialize the list to store augmented images\n",
        "    augmented_images = [original_image_rgb]\n",
        "\n",
        "    # Flip the image horizontally\n",
        "    # flipped_image = cv2.flip(original_image_rgb, 1)\n",
        "    # augmented_images.append(flipped_image)\n",
        "\n",
        "    # Rotate the image by custom angles\n",
        "    rotation_angles = np.arange(-15,16,10)\n",
        "    for angle in rotation_angles:\n",
        "        rotated_image = PILImage.fromarray(original_image_rgb)\n",
        "        rotated_image = rotated_image.rotate(angle)\n",
        "        rotated_image = np.array(rotated_image)\n",
        "        augmented_images.append(rotated_image)\n",
        "\n",
        "    # GaussianBlur the image\n",
        "\n",
        "    blurred_image = cv2.GaussianBlur(original_image_rgb, (5 , 5), 0)\n",
        "    augmented_images.append(blurred_image)\n",
        "\n",
        "    return augmented_images"
      ],
      "metadata": {
        "id": "0syIY18HKznq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mediapipe Functions"
      ],
      "metadata": {
        "id": "VMojh0UgMq4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grabbing the Holistic Model from Mediapipe and\n",
        "# Initializing the Model\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic_model = mp_holistic.Holistic(\n",
        "\tmin_detection_confidence=0.1,\n",
        "\tmin_tracking_confidence=0.1\n",
        ")\n",
        "\n",
        "# Initializing the drawing utils for drawing the facial landmarks on image\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing_styles = mp.solutions.drawing_styles"
      ],
      "metadata": {
        "id": "Nm4QzItEK2JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading images and labels from the Folder"
      ],
      "metadata": {
        "id": "YTmxh_FyNDmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace the folder dir with r\"<your path>\\train\"\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "Image = []\n",
        "Label = []\n",
        "folder_dir = r\"C:\\Users\\Manishit\\Documents\\Academia\\Projects\\Sign_Language_Recognition\\American_Sign_Language_Letters_Yolo\\train\"\n",
        "count = 0\n",
        "for k in tqdm(os.listdir(folder_dir), desc='Processing images', unit='items'):\n",
        "    count += 1\n",
        "    if k.endswith('.jpg'):\n",
        "        i = cv2.imread(os.path.join(folder_dir,k))\n",
        "        i = cv2.flip(i,1)\n",
        "        Image.append(i)\n",
        "        Label.append(k[0])"
      ],
      "metadata": {
        "id": "tYsKwjqsLSWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expanding the dataset by augmentation"
      ],
      "metadata": {
        "id": "qIjl2jpMNZ8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SI = []\n",
        "SL = []\n",
        "for num,img in tqdm(enumerate(Image)):\n",
        "    Au = augment_image(img)\n",
        "    SI.extend(Au)\n",
        "    SL.extend([Label[num]]*len(Au))\n",
        "Image = SI"
      ],
      "metadata": {
        "id": "-X4g5iutLYu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting mediapipe landmarks for each data point"
      ],
      "metadata": {
        "id": "xqWC2NJkNszL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "H = []\n",
        "label = []\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.2) as hands:\n",
        "  for num,image in tqdm(enumerate(Image),desc = 'Creating Features',unit = 'Items'):\n",
        "    # Read an image, flip it around y-axis for correct handedness output (see\n",
        "    # above).\n",
        "    if not((image[:,:,0] == image[:,:,1]).all()*(image[:,:,1] == image[:,:,2]).all()):\n",
        "        #Label.append(SL[num])\n",
        "        label.append(Label[num])\n",
        "        image = cv2.flip(image, 1)\n",
        "        # Convert the BGR image to RGB before processing.\n",
        "        results = hands.process(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n",
        "        if results.multi_handedness:\n",
        "            if results.multi_handedness[0].classification[0].label == 'Left':\n",
        "                results = hands.process(cv2.flip(cv2.cvtColor(image,cv2.COLOR_BGR2RGB),1))\n",
        "\n",
        "        H.append(handland(results))\n",
        "\n",
        "H_new = [i for i in H if np.sum(i**2) != 0]\n",
        "Label_new = [label[i] for i in range(len(label)) if np.sum(H[i]**2) != 0]\n",
        "len(Label_new)"
      ],
      "metadata": {
        "id": "NIzTqITZLZwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the processed dataset to put into FFNN"
      ],
      "metadata": {
        "id": "W4E9G5p3N5PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('Train_X_aug.txt',H_new)\n",
        "np.savetxt('Train_Y_aug.txt',Label_new,fmt = \"%s\")"
      ],
      "metadata": {
        "id": "hoaaNHoJLsPr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}