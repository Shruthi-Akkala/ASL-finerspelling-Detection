{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Dependencies"
      ],
      "metadata": {
        "id": "fIVjoPjsMPgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import time\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from tqdm import tqdm\n",
        "from keras import regularizers"
      ],
      "metadata": {
        "id": "JR_5cjUbKrm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining necessary functions"
      ],
      "metadata": {
        "id": "q7PTxhv4MSNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Handland takes the mediapipe output landmarks, collates all the coordinates and flattens it to create an array of size 63\n",
        "\n",
        "def handland(results):\n",
        "    hl = np.array([[landmark.x,landmark.y,landmark.z] for landmark in results.multi_hand_landmarks[0].landmark]).flatten() if results.multi_hand_landmarks else np.zeros(63)\n",
        "    return hl\n",
        "\n",
        "def handscore(results):\n",
        "    sc = results.multi_handedness[0].classification[0].score\n",
        "    return sc\n",
        "\n",
        "#Choose is basically the combined model. We have three different custom models and the combination of the three seems to be working better.\n",
        "\n",
        "def choose(a,b,c):\n",
        "    if a != '':\n",
        "        a = ord(a)\n",
        "        b = ord(b)\n",
        "        c = ord(c)\n",
        "        if (b-c) == 0:\n",
        "            return chr(b)\n",
        "        else:\n",
        "            return chr(a)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "\n",
        "#Augmentation Function\n",
        "\n",
        "from PIL import Image as PILImage\n",
        "def augment_image(image):\n",
        "    # Read the original image\n",
        "    original_image = image\n",
        "\n",
        "    # Convert image from BGR to RGB\n",
        "    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Initialize the list to store augmented images\n",
        "    augmented_images = [original_image_rgb]\n",
        "\n",
        "    # Flip the image horizontally\n",
        "    # flipped_image = cv2.flip(original_image_rgb, 1)\n",
        "    # augmented_images.append(flipped_image)\n",
        "\n",
        "    # Rotate the image by custom angles\n",
        "    rotation_angles = np.arange(-15,16,10)\n",
        "    for angle in rotation_angles:\n",
        "        rotated_image = PILImage.fromarray(original_image_rgb)\n",
        "        rotated_image = rotated_image.rotate(angle)\n",
        "        rotated_image = np.array(rotated_image)\n",
        "        augmented_images.append(rotated_image)\n",
        "\n",
        "    # GaussianBlur the image\n",
        "\n",
        "    blurred_image = cv2.GaussianBlur(original_image_rgb, (5 , 5), 0)\n",
        "    augmented_images.append(blurred_image)\n",
        "\n",
        "    return augmented_images"
      ],
      "metadata": {
        "id": "0syIY18HKznq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mediapipe Functions"
      ],
      "metadata": {
        "id": "VMojh0UgMq4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grabbing the Holistic Model from Mediapipe and\n",
        "# Initializing the Model\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic_model = mp_holistic.Holistic(\n",
        "\tmin_detection_confidence=0.1,\n",
        "\tmin_tracking_confidence=0.1\n",
        ")\n",
        "\n",
        "# Initializing the drawing utils for drawing the facial landmarks on image\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing_styles = mp.solutions.drawing_styles"
      ],
      "metadata": {
        "id": "Nm4QzItEK2JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the model weights"
      ],
      "metadata": {
        "id": "0DDT2bN1PrgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We load the weights for all the 3 custom models here\n",
        "\n",
        "#The efficiency of the model is decreasing with increase in model number i.e. Model 1 is suppposed to be the best and Model 3 is worst\n",
        "#Model 1 is trained on the custom train dataset we created\n",
        "#Model 2 is trained on the Augmented Dataset we obtained from the Yolo  Train Data\n",
        "#Model 3 is trained on a much bigger dataset + augmentation, but the quality of the dataset was not up to the mark and it often used wrong signs\n",
        "\n",
        "def baseline_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(46, input_dim=63, activation='relu', kernel_regularizer=regularizers.l1(0.01)))\n",
        "  model.add(Dense(26, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model1 = baseline_model()\n",
        "model1.load_weights(\"wcdata_2layers.h5\")\n",
        "model2 = baseline_model()\n",
        "model2.load_weights(\"augdata_2layer.h5\")\n",
        "model3 = baseline_model()\n",
        "model3.load_weights(\"bigdataASLNN26.h5\")"
      ],
      "metadata": {
        "id": "SIJXb8dZPWW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming from webcam and using the combined model to predict the sign"
      ],
      "metadata": {
        "id": "dN8hXVUvQkmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are in this version of code using combined model to generate the output. But in order to change it to any particular model, change the second argument of cv2.putText to sout1,sout2 or sout3 accordingly.\n",
        "\n",
        "L = []\n",
        "Con = []\n",
        "\n",
        "# For webcam input:\n",
        "cap = cv2.VideoCapture(0)\n",
        "with mp_hands.Hands(\n",
        "    max_num_hands = 1,\n",
        "    model_complexity=0,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5) as hands:\n",
        "  previousTime = 0\n",
        "  currentTime = 0\n",
        "  while cap.isOpened():\n",
        "    success, image = cap.read()\n",
        "    if not success:\n",
        "      print(\"Ignoring empty camera frame.\")\n",
        "      # If loading a video, use 'break' instead of 'continue'.\n",
        "      continue\n",
        "\n",
        "    # To improve performance, optionally mark the image as not writeable to\n",
        "    # pass by reference.\n",
        "    image = cv2.flip(image,1)\n",
        "    image.flags.writeable = False\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(image)\n",
        "    check = 0\n",
        "    if results.multi_hand_landmarks:\n",
        "        H = handland(results)\n",
        "        #H = feature_create(H)\n",
        "        H = np.reshape(H,(1,63))\n",
        "        check = 1\n",
        "\n",
        "    if check == 1:\n",
        "        sout1 = chr(np.argmax(model1(H)) + 65)\n",
        "        sout2 = chr(np.argmax(model2(H)) + 65)\n",
        "        sout3 = chr(np.argmax(model3(H)) + 65)\n",
        "        Con.append(handscore(results))\n",
        "        L.append(sout)\n",
        "    else:\n",
        "        sout1 = ''\n",
        "        sout2 = ''\n",
        "        sout3 = ''\n",
        "\n",
        "\n",
        "    # Calculating the FPS\n",
        "    currentTime = time.time()\n",
        "    fps = 1 / (currentTime-previousTime)\n",
        "    previousTime = currentTime\n",
        "\n",
        "    # Displaying FPS on the image\n",
        "    image = cv2.flip(image,1)\n",
        "    cv2.putText(image, choose(sout1,sout2,sout3) , (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
        "    image = cv2.flip(image,1)\n",
        "\n",
        "    # Draw the hand annotations on the image.\n",
        "    image.flags.writeable = True\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    if results.multi_hand_landmarks:\n",
        "      for hand_landmarks in results.multi_hand_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            image,\n",
        "            hand_landmarks,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "    # Flip the image horizontally for a selfie-view display.\n",
        "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
        "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
        "      break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "e7yORr6WPPch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}